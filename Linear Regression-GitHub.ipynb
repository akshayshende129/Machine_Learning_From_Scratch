{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:46:11.953034Z",
     "start_time": "2019-09-04T17:46:05.599170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy** : Numerical Computation for faster execution  \n",
    "\n",
    "**sklearn** : Popular ML library available in Python  \n",
    "\n",
    "**train_test_split** : This is used to split the available data into 2 parts,  \n",
    "- 1<sup><b>st</b></sup> into Train Set \n",
    "- 2<sup><b>nd</b></sup> into Test test\n",
    "\n",
    "**sklearn.linear_model** : In the package you will find all the linear model available. We are using in-built LinearRegression library for checking purpose w.r.t our scratch algorithm.  \n",
    "\n",
    "**sklearn.metrics** : In the package you will find all the evaulation of model i.e how well you are model is performing. We will use Mean Squared Error.\n",
    "\n",
    "What is Mean Squared Error?  \n",
    "- Sum of the difference between actual value and your predicted value and squaring the to difference term.  \n",
    "- Divide it with the number of value a.k.a mean\n",
    "*Why we do the square the difference?*\n",
    "    - Squaring always gives a positive value, so the sum will not be zero. \n",
    "    - Squaring emphasizes larger differences — a feature that turns out to be both good and bad (think of the effect outliers have).\n",
    "- Formula:  \n",
    "`` SUM((actual - predicted)^2) / COUNT(actual_values)``\n",
    "- Larger the value worst your model and vice versa\n",
    "\n",
    "**NOTE : We are not going to optimize the model performance here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "- Simple relation between dependent and independent variable.\n",
    "\n",
    "- Simple Linear Regression:  \n",
    "    `y = mx + b`  \n",
    "        x = Random Varible  \n",
    "        m = Slope = rise/run  \n",
    "        b = y - intercept [ where x = 0 ]  \n",
    "    \n",
    "- Multiple Linear Regression:  \n",
    "    `Y = B1X1 + B2X2 + ..... + BnXn`  \n",
    "        Bi : Beta Parameters/Weights/Gradients    \n",
    "        Xi : Features/Columns/Independent  \n",
    "        Y : Dependent Variable  \n",
    "\n",
    "- How to find weights/beta parameters?  \n",
    "    <B>Using Gadient Descent Method.</B>  \n",
    "    > What is Gradient Descent Method?\n",
    "        Imagine a bowl which in U in a shape. The best optimal value are at the bottom.\n",
    "        Now how to get over there. \n",
    "        Gradient Descent implementation steps\n",
    "\n",
    "        Step1: Initialize parameters (weight and bias) with random value or simply zero.Also initialize Learning rate.\n",
    "\n",
    "        Step2: Calculate Cost function (J)\n",
    "\n",
    "        Step3: Take Partial Derivatives of the cost function with respect to weights and biases(dW,db).\n",
    "\n",
    "        Step4: Update Parameter values as:\n",
    "\n",
    "            Wnew =  W – learning rate * dW\n",
    "            bnew = b – learning rate * db\n",
    "        \n",
    "        Step5: Repeat step2 to step 4 till n no of iterations. With each iteration the value of cost function will                   progressively decrease and eventually become flat value.\n",
    "        \n",
    "*Let Breakdown further in terms of coding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:46:11.993926Z",
     "start_time": "2019-09-04T17:46:11.979967Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression_Scratch:\n",
    "    \n",
    "    intercept = None\n",
    "    learning_rate = None\n",
    "    convergence = None\n",
    "    grads = None\n",
    "    x = None\n",
    "    y = None\n",
    "    \n",
    "    def __init__(self, intercept = False,alpha = 0.001,convergence = 1e-4):\n",
    "        self.intercept = intercept\n",
    "        self.learning_rate = alpha\n",
    "        self.convergence = convergence\n",
    "        \n",
    "    \n",
    "    def fit(self,x, y):\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        if x is not None and y is not None:\n",
    "            if self.intercept == True:\n",
    "                interept_value = np.zeros(x.shape[0])\n",
    "                self.x = np.insert(self.x,0,interept_value,axis=1)\n",
    "            self.grads = np.random.randn(self.x.shape[0],self.x.shape[1])\n",
    "            while True:\n",
    "                self.grads,loss = self._gradient_descent(self.grads)\n",
    "                new_grads = self.grads - self.learning_rate * self.grads\n",
    "                \n",
    "                if np.sum(abs(new_grads - self.grads)) < self.convergence:\n",
    "                    print(\"Converged\")\n",
    "                    self.grads = new_grads\n",
    "                    break\n",
    "                self.grads = new_grads\n",
    "        \n",
    "        return self\n",
    "\n",
    "                \n",
    "    def _gradient_descent(self,grads):\n",
    "        x = np.array(self.x)\n",
    "        y = np.array(self.y)\n",
    "        \n",
    "        pred = np.sum(x.dot(grads.T),axis=1,keepdims=True)\n",
    "        sqPred = np.power((y - pred),2)\n",
    "        loss = np.sum(sqPred)/x.shape[0]\n",
    "        \n",
    "        return grads,loss\n",
    "    \n",
    "    def predict(self,X):\n",
    "        if self.intercept == True:\n",
    "            interept_value = np.zeros(X.shape[0])\n",
    "            X = np.insert(X,0,interept_value,axis=1)\n",
    "        x = np.array(X)\n",
    "        pred = np.sum(x.dot(self.grads.T),axis=1,keepdims=True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown\n",
    "\n",
    "## Variable Used :  \n",
    "|  **Variable** \t|                                                                                                    **Meaning**                                                                                                    \t| **Values**                          \t|\n",
    "|:-------------:\t|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:\t|-------------------------------------\t|\n",
    "| intercept     \t| Weather we want to fit the intercept or not.                                                                                                                                                                      \t| True or False                       \t|\n",
    "| learning_rate \t| Amount by which you want to increase or decrease <br>  the learning of weights                                                                                                                                    \t| Generally in range of 0.1 to 0.0001 \t|\n",
    "| convergence   \t| Gradient Descent makes very small changes <br> in your objective function is called convergence, <br> which doesn't mean it has reached the optimal result <br> (but it is really quite quite near, if not on it) \t| Generally 0.001                     \t|\n",
    "| grads         \t| Variable to store the weights of the parameters <br> when cost function values are updated.                                                                                                                       \t|                                     \t|\n",
    "| x             \t| Independent Features                                                                                                                                                                                              \t|                                     \t|\n",
    "| y             \t| Dependent Features                                                                                                                                                                                                \t|                                     \t|  \n",
    "\n",
    "## Functions :\n",
    "**init** : Used to initialize the values value when you call the constructor. We have initialize the value of learning_rate, convergence & intercept.  \n",
    "**fit** : We use it to train the model. Basically finding the weights/beta parameter for the feature.  \n",
    "**gradient** : Here we have implemented gradient descent which in turn help to find the optimal value for our features.  \n",
    "**predict** : After finding the optimal parameter we use those value of the predict new o/p.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA - Preparation\n",
    "Creating own data to test\n",
    "- Creating 10 **features/independent/columns/X** with 10000 **values/rows** along with 1 **target/dependent/Y**\n",
    "- This are all generated using numpy.random.rand function of NUMPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T17:46:13.283669Z",
     "start_time": "2019-09-04T17:46:13.232805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10) (10000, 1)\n",
      "(6700, 10) (6700, 1)\n",
      "(3300, 10) (3300, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10000,10)\n",
    "y = np.random.randn(10000,1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.33, random_state = 123)\n",
    "print(x.shape, y.shape)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent CODE Breakdown\n",
    "        1. grads = np.random.randn(x.shape[0],x.shape[1])\n",
    "            Here we have taken random value for the initial values. We created a numpy matrix of size x_rows*x_columns\n",
    "        2. x = np.array(x) # Converting passed values to numpy array\n",
    "        3. y = np.array(y) # Converting passed values to numpy array\n",
    "        4. pred = np.sum(x.dot(grads.T),axis=1,keepdims=True)\n",
    "            Here we are predicting the value y using the randomly initialize weights/gradients/beta values.\n",
    "            It simply a dot product of the X/independent/features/columns with the gradients/weights/beta value.\n",
    "            x.dot(grads.T) : This gives us matrix multiplication return for all values at once.\n",
    "                             Matrix multiplication has one basic rule i.e., no of columns of X must be equal to rows of y\n",
    "                             Suppose X = (3,4) then y must = (4,5), resultant matrix will be of size (3,5)\n",
    "                             In general X = (n,m) and y = (m,p) resultant matrix will be (n,p) size matrix\n",
    "            axis : 1 means to use values of columns not row wise\n",
    "            keepdims : Basically in python we it always return as (n,) \n",
    "                       without any dimension and we want to keep the dimensions for computation.\n",
    "         5. sqPred = np.power((y - pred),2)\n",
    "                     (ACTUAL - PREDICTED) ** 2\n",
    "             np.power : This function is to raise any value to the prescribed power, here we are raisin value to power of 2\n",
    "                        Why only to power of 2, not 3 or 4?\n",
    "                        Power of 1 will give you same value, power of 2 will get you rid of the negative value elimination.\n",
    "                        Power of 3 will give you again negative and power of 4 will get rid of negative values.\n",
    "                        Example: (2-3)^1 = -1, |2-2| = 0, (2-3)^2 = 1 -> The benefits of squaring include:\n",
    "                        - Squaring always gives a positive value, so the sum will not be zero.\n",
    "                        - Squaring emphasizes larger differences — a feature that turns out to be both good and bad (think                           of the effect outliers have).\n",
    "          6. loss = np.sum(sqPred)/x.shape[0]\n",
    "             np.sum : Add all of the values. \n",
    "             x.shape[0] : gives you total row/point of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grads,x,y):\n",
    "        grads = np.random.randn(x.shape[0],x.shape[1])\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        print('Grads : \\n' , pd.DataFrame(grads))\n",
    "        \n",
    "        print('Init Wi : ' , grads.shape, ' : ', grads.T.shape)\n",
    "        print('X Shape : ' , x.shape)\n",
    "        print('Y Shape : ', y.shape)\n",
    "        \n",
    "        pred = np.sum(x.dot(grads.T),axis=1,keepdims=True)\n",
    "        print('PRED : ' , pred.shape , ' :\\n' , pd.DataFrame(pred))\n",
    "        \n",
    "        sqPred = np.power((y - pred),2)\n",
    "        print('SQR Error : \\n' , pd.DataFrame(sqPred))\n",
    "        \n",
    "        loss = np.sum(sqPred)/x.shape[0]\n",
    "        print('LOSS : ' , loss)\n",
    "        \n",
    "        return grads, loss\n",
    "        \n",
    "grads = np.random.randn(x.shape[0],x.shape[1])        \n",
    "gradient_descent(grads, x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence : As you train your algorithm using gradient descent approach the new weights which are obtained by multiplying the gradient with learning parameters and subtracting it from weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence = 1e-10\n",
    "learning_rate = 0.001\n",
    "loss = []\n",
    "grads = np.random.randn(x.shape[0],x.shape[1])\n",
    "iterations = 1\n",
    "while True:\n",
    "    grads,error = gradient_descent(grads,x,y)\n",
    "    new_grads = grads - learning_rate * grads\n",
    "\n",
    "    if np.sum(abs(new_grads - grads)) < convergence:\n",
    "        print(\"Converged\")\n",
    "        #grads = new_grads\n",
    "        #loss.append(error)\n",
    "        break\n",
    "        \n",
    "    if iterations % 100 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" %(iterations, error))\n",
    "        \n",
    "    grads = new_grads\n",
    "    iterations += 1\n",
    "    loss.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T15:57:02.759824Z",
     "start_time": "2019-09-03T15:56:31.850236Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression_Scratch(intercept = False, alpha = 0.001)\n",
    "lr.fit(train_x,train_y)\n",
    "pred = lr.predict(test_x)\n",
    "mean_squared_error(test_y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T15:57:03.054037Z",
     "start_time": "2019-09-03T15:57:03.047060Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T15:57:03.481892Z",
     "start_time": "2019-09-03T15:57:03.185726Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr_sk = LinearRegression()\n",
    "lr_sk.fit(train_x,train_y)\n",
    "pred_sk = lr.predict(test_x)\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "mean_squared_error(test_y,pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T10:11:19.293694Z",
     "start_time": "2019-09-03T10:11:19.263285Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T10:16:12.286923Z",
     "start_time": "2019-09-03T10:16:11.172909Z"
    }
   },
   "outputs": [],
   "source": [
    "convergence = 1e-10\n",
    "learning_rate = 0.001\n",
    "loss = []\n",
    "grads = np.random.randn(x.shape[0],x.shape[1])\n",
    "iterations = 1\n",
    "while True:\n",
    "    grads,error = gradient_descent(grads,x,y)\n",
    "    new_grads = grads - learning_rate * grads\n",
    "\n",
    "    if np.sum(abs(new_grads - grads)) < convergence:\n",
    "        print(\"Converged\")\n",
    "        #grads = new_grads\n",
    "        #loss.append(error)\n",
    "        break\n",
    "        \n",
    "    if iterations % 100 == 0:\n",
    "        print(\"Iteration: %d - Error: %.4f\" %(iterations, error))\n",
    "        \n",
    "    grads = new_grads\n",
    "    iterations += 1\n",
    "    loss.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T10:51:24.588719Z",
     "start_time": "2019-09-03T10:51:23.752005Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(x=loss,y=range(len(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T10:16:17.160960Z",
     "start_time": "2019-09-03T10:16:17.148170Z"
    }
   },
   "outputs": [],
   "source": [
    "len(loss),np.argmax(loss),np.argmin(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:49:14.640422Z",
     "start_time": "2019-09-03T05:49:14.633440Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y = b0 + b1 * x\n",
    "b0 = Σy − b1 Σx * N\n",
    "b1 =  N Σ(xy) − Σx Σy/ N Σ(x2) − (Σx)2\n",
    "\"\"\"\n",
    "\n",
    "def simpleLinearRegression_Fit(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    if x.shape[0] == y.shape[0]:\n",
    "        \n",
    "        x_2 = np.power(x,2)\n",
    "        xy = np.multiply(x,y)\n",
    "        x_sum = np.sum(x)\n",
    "        y_sum = np.sum(y)        \n",
    "        x_2_sum = np.sum(x_2)\n",
    "        xy_sum = np.sum(xy)\n",
    "        vector_space = x.shape[0]\n",
    "        \n",
    "        b1 = (vector_space * xy_sum - (x_sum * y_sum)) / (vector_space * x_2_sum - (x_sum)**2)\n",
    "        b0 = (y_sum - (b1 * x_sum)) / vector_space\n",
    "        return b0,b1\n",
    "\n",
    "def predict(b0,b1,x):\n",
    "    x = np.array(x)\n",
    "    return b0 + (b1 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:49:15.455242Z",
     "start_time": "2019-09-03T05:49:15.438290Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [2,3,5,7,9]\n",
    "y = [4,5,7,10,15]\n",
    "x_test = [3,4,12,12,23]\n",
    "b0,b1 = simpleLinearRegression_Fit(x,y)\n",
    "predict(b0,b1,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "901px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
